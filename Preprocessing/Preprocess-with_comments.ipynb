{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ejer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scraping_class\n",
    "import requests\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdrivermanager import GeckoDriverManager\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "\n",
    "# NLTK: A basic, popular NLP package. Find many examples of applications at https://www.nltk.org/book/\n",
    "# Install guide: https://www.nltk.org/install.html\n",
    "import nltk\n",
    "nltk.download('punkt') # you will probably need to do this\n",
    "nltk.download('wordnet') # and this\n",
    "nltk.download('stopwords') # aand this\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Vader Lexicon for sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Lexicons for sentiment analysis\n",
    "from vaderSentiment import vaderSentiment\n",
    "from afinn import Afinn\n",
    "\n",
    "# to display images in notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,\n",
    "                              rrulewrapper, RRuleLocator, drange)\n",
    "import numpy as np\n",
    "import datetime\n",
    "porter = nltk.PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import scraping_class\n",
    "import requests\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdrivermanager import GeckoDriverManager\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "\n",
    "# NLTK: A basic, popular NLP package. Find many examples of applications at https://www.nltk.org/book/\n",
    "# Install guide: https://www.nltk.org/install.html\n",
    "import nltk\n",
    "nltk.download('punkt') # you will probably need to do this\n",
    "nltk.download('wordnet') # and this\n",
    "nltk.download('stopwords') # aand this\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Vader Lexicon for sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Lexicons for sentiment analysis\n",
    "from vaderSentiment import vaderSentiment\n",
    "from afinn import Afinn\n",
    "\n",
    "# to display images in notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "stemmer = SnowballStemmer(\"danish\") \n",
    "stop_words_list = nltk.corpus.stopwords.words('danish')\n",
    "\n",
    "import lemmy\n",
    "\n",
    "# Create an instance of the standalone lemmatizer.\n",
    "lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Data.csv\") # read the initial dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "   \n",
    "    remowe_html = re.sub(r'<.*?>','', text) # remowes any links\n",
    "    punct_removed_2 = re.sub(r'[^\\w\\s]','',remowe_html) # remows punctuation \n",
    "    sent = [i for i in nltk.tokenize.word_tokenize(punct_removed_2) if i not in stop_words_list]# tokeniser the words, and remowes the stop words\n",
    "    \n",
    "    for i in sent: # only stem words with that begins with lowercase, thus hoping to not stem names\n",
    "        liste=[]\n",
    "        if i[0].isupper()==False:\n",
    "            x=stemmer.stem(i)\n",
    "        else:\n",
    "            x=i\n",
    "        liste.append(x)\n",
    "    final=[lemmatizer.lemmatize(\"v\", i) for i in sent] # lematise the words, using danish lematiser\n",
    "   \n",
    "    numbers=[re.sub(r'\\d+', 'Unikttal' ,i[0]) for i in final] # findding all numbers, and returning a unique number\n",
    "    \n",
    "    ret=[i.lower() for i in numbers] # lowercasse evertyning to make less words\n",
    "  \n",
    "    return ret # returns the propreocces words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>comments</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ekstrabladet.dk/nyheder/politik/danskp...</td>\n",
       "      <td>1952</td>\n",
       "      <td>Statsministeren åbner for muligt påbud om mund...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Statsminister Mette Frederiksen (S) åbner for,...</td>\n",
       "      <td>7. aug. 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ekstrabladet.dk/nyheder/politik/danskp...</td>\n",
       "      <td>448</td>\n",
       "      <td>Mette F. afviser at åbne for nattelivet i august</td>\n",
       "      <td>Mette Frederiksen (S) siger, at hun ikke kan s...</td>\n",
       "      <td>Statsminister Mette Frederiksen (S) siger, at ...</td>\n",
       "      <td>7. aug. 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ekstrabladet.dk/flash/filmogtv/daarlig...</td>\n",
       "      <td>34</td>\n",
       "      <td>Dårligt nyt til 'Venner'-fans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Der var gode nyheder til fans af serien 'Frien...</td>\n",
       "      <td>7. aug. 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ekstrabladet.dk/nyheder/samfund/flemmi...</td>\n",
       "      <td>227</td>\n",
       "      <td>Flemming dødtræt af Elgiganten: Gør det aldrig!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Per, Mikkel, Jeppe, Allan, Zanshi og Jasmin. F...</td>\n",
       "      <td>7. aug. 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ekstrabladet.dk/nyheder/samfund/venstr...</td>\n",
       "      <td>27</td>\n",
       "      <td>Venstre vil have håndtrykket tilbage, hvis det...</td>\n",
       "      <td>Virolog mener dog ikke, at tidspunktet er det ...</td>\n",
       "      <td>Hvis du synes, det var svært at finde ud af, h...</td>\n",
       "      <td>7. aug. 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24083</th>\n",
       "      <td>https://ekstrabladet.dk/flash/filmogtv/her-er-...</td>\n",
       "      <td>25</td>\n",
       "      <td>Her er Britt Bendixens afløser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En større operation i hjertet tvang for en mån...</td>\n",
       "      <td>19. aug. 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24084</th>\n",
       "      <td>https://ekstrabladet.dk/flash/udlandkendte/kar...</td>\n",
       "      <td>33</td>\n",
       "      <td>Kardashian om hudløst ærligt billede: - Jeg el...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Man kan ikke beskylde nogen af Kardashian-søst...</td>\n",
       "      <td>19. aug. 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24085</th>\n",
       "      <td>https://ekstrabladet.dk/sport/fodbold/udenland...</td>\n",
       "      <td>106</td>\n",
       "      <td>Hug til Eriksen: - Ikke i nærheden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Christian Eriksen er tidligere blevet sammenli...</td>\n",
       "      <td>19. aug. 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24086</th>\n",
       "      <td>https://ekstrabladet.dk/biler/har-reddet-milli...</td>\n",
       "      <td>19</td>\n",
       "      <td>Har reddet millioner af liv: Denne geniale bil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I dag er moderne biler meget sikre og alverden...</td>\n",
       "      <td>19. aug. 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24087</th>\n",
       "      <td>https://ekstrabladet.dk/nyheder/politik/danskp...</td>\n",
       "      <td>514</td>\n",
       "      <td>Stempler Trumps tvivl om Danmarks-tur: 'Uforsk...</td>\n",
       "      <td>Det er kun årsager som dødsfald eller en inter...</td>\n",
       "      <td>Brud på diplomatiet, en fornærmelse, uforskammet.</td>\n",
       "      <td>19. aug. 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24088 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  comments  \\\n",
       "0      https://ekstrabladet.dk/nyheder/politik/danskp...      1952   \n",
       "1      https://ekstrabladet.dk/nyheder/politik/danskp...       448   \n",
       "2      https://ekstrabladet.dk/flash/filmogtv/daarlig...        34   \n",
       "3      https://ekstrabladet.dk/nyheder/samfund/flemmi...       227   \n",
       "4      https://ekstrabladet.dk/nyheder/samfund/venstr...        27   \n",
       "...                                                  ...       ...   \n",
       "24083  https://ekstrabladet.dk/flash/filmogtv/her-er-...        25   \n",
       "24084  https://ekstrabladet.dk/flash/udlandkendte/kar...        33   \n",
       "24085  https://ekstrabladet.dk/sport/fodbold/udenland...       106   \n",
       "24086  https://ekstrabladet.dk/biler/har-reddet-milli...        19   \n",
       "24087  https://ekstrabladet.dk/nyheder/politik/danskp...       514   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Statsministeren åbner for muligt påbud om mund...   \n",
       "1       Mette F. afviser at åbne for nattelivet i august   \n",
       "2                          Dårligt nyt til 'Venner'-fans   \n",
       "3        Flemming dødtræt af Elgiganten: Gør det aldrig!   \n",
       "4      Venstre vil have håndtrykket tilbage, hvis det...   \n",
       "...                                                  ...   \n",
       "24083                     Her er Britt Bendixens afløser   \n",
       "24084  Kardashian om hudløst ærligt billede: - Jeg el...   \n",
       "24085                 Hug til Eriksen: - Ikke i nærheden   \n",
       "24086  Har reddet millioner af liv: Denne geniale bil...   \n",
       "24087  Stempler Trumps tvivl om Danmarks-tur: 'Uforsk...   \n",
       "\n",
       "                                                subtitle  \\\n",
       "0                                                    NaN   \n",
       "1      Mette Frederiksen (S) siger, at hun ikke kan s...   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4      Virolog mener dog ikke, at tidspunktet er det ...   \n",
       "...                                                  ...   \n",
       "24083                                                NaN   \n",
       "24084                                                NaN   \n",
       "24085                                                NaN   \n",
       "24086                                                NaN   \n",
       "24087  Det er kun årsager som dødsfald eller en inter...   \n",
       "\n",
       "                                                    body           date  \n",
       "0      Statsminister Mette Frederiksen (S) åbner for,...   7. aug. 2020  \n",
       "1      Statsminister Mette Frederiksen (S) siger, at ...   7. aug. 2020  \n",
       "2      Der var gode nyheder til fans af serien 'Frien...   7. aug. 2020  \n",
       "3      Per, Mikkel, Jeppe, Allan, Zanshi og Jasmin. F...   7. aug. 2020  \n",
       "4      Hvis du synes, det var svært at finde ud af, h...   7. aug. 2020  \n",
       "...                                                  ...            ...  \n",
       "24083  En større operation i hjertet tvang for en mån...  19. aug. 2019  \n",
       "24084  Man kan ikke beskylde nogen af Kardashian-søst...  19. aug. 2019  \n",
       "24085  Christian Eriksen er tidligere blevet sammenli...  19. aug. 2019  \n",
       "24086  I dag er moderne biler meget sikre og alverden...  19. aug. 2019  \n",
       "24087  Brud på diplomatiet, en fornærmelse, uforskammet.  19. aug. 2019  \n",
       "\n",
       "[24088 rows x 6 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=data.copy() # makes a copy of the dataframe\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 24030/24088 [19:07<00:02, 22.80it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "24031",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-792f822c2253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4404\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4405\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 24031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 24030/24088 [19:21<00:02, 22.80it/s]"
     ]
    }
   ],
   "source": [
    "x=len(df1)\n",
    "list=[]\n",
    "\n",
    "for i  in tqdm(range(x)):\n",
    "    \n",
    "    df1.dropna(subset = [\"title\"], inplace=True) # droppes articles with missing in the tile and reset index\n",
    "    df=df1.reset_index(drop=True)\n",
    "    \n",
    "    x=preprocess(df['title'][i]) # preporcces and append to list\n",
    "    list.append(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(df)\n",
    "df2['Preprocces Title']=pd.Series(list) # append the preproces data to a new collum \n",
    "df2.to_csv(\"preprced.csv\",index=False) #saves it to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest is just not  working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "    \n",
    "#     print (person_list)\n",
    "\n",
    "#text = \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = get_human_names(text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_human_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-13a825f2171d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_human_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_human_names' is not defined"
     ]
    }
   ],
   "source": [
    "print(get_human_names(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "   \n",
    "    remowe_html = re.sub(r'<.*?>','', text) # re\n",
    "    punct_removed_2 = re.sub(r'[^\\w\\s]','',remowe_html)\n",
    "    sent = [i for i in nltk.tokenize.word_tokenize(punct_removed_2) if i not in stop_words_list]# tokeniser ordet\n",
    "    for i in sent:\n",
    "        liste=[]\n",
    "        if i.isupper()==False:\n",
    "            x=stemmer.stem(i)\n",
    "        else:\n",
    "            i=x\n",
    "        liste.append(x)\n",
    "            \n",
    "    final=[lemmatizer.lemmatize(\"n\", i) for i in liste]\n",
    "   \n",
    "    numbers=[re.sub(r'\\d+', 'Unikttal' ,i[0]) for i in sent]\n",
    "    \n",
    "    ret=[i.lower() for i in numbers]\n",
    "  \n",
    "    return ret # return a list of stems/lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "   \n",
    "    remowe_html = re.sub(r'<.*?>','', text) # re\n",
    "    punct_removed_2 = re.sub(r'[^\\w\\s]','',remowe_html)\n",
    "    sent = [i for i in nltk.tokenize.word_tokenize(punct_removed_2) if i not in stop_words_list]# tokeniser ordet\n",
    "    \n",
    "    for i in sent:\n",
    "        liste=[]\n",
    "        if i[0].isupper()==False:\n",
    "            x=stemmer.stem(i)\n",
    "        else:\n",
    "            x=i\n",
    "        liste.append(x)\n",
    "    final=[lemmatizer.lemmatize(\"v\", i) for i in sent]\n",
    "   \n",
    "    numbers=[re.sub(r'\\d+', 'Unikttal' ,i[0]) for i in final]\n",
    "    print(numbers)\n",
    "    ret=[i.lower() for i in numbers]\n",
    "  \n",
    "    return ret # return a list of stems/lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statsministeren', 'åbner', 'muligt', 'påbud', 'mundbind']\n",
      "['Statsministeren', 'åbner', 'muligt', 'påbud', 'mundbind']\n",
      "['Mette', 'F', 'afviser', 'åbne', 'nattelivet', 'august']\n",
      "['Mette', 'F', 'afviser', 'åbne', 'nattelivet', 'august']\n",
      "['Dårligt', 'nyt', 'Vennerfans']\n",
      "['Dårligt', 'nyt', 'Vennerfans']\n",
      "['Flemming', 'dødtræt', 'Elgiganten', 'Gør', 'aldrig']\n",
      "['Flemming', 'dødtræt', 'Elgiganten', 'Gør', 'aldrig']\n",
      "['Venstre', 'håndtrykket', 'tilbage', 'kan', 'lade', 'gøre']\n",
      "['Venstre', 'håndtrykket', 'tilbage', 'kan', 'lade', 'gøre']\n",
      "['Bizar', 'opladning', 'Derfor', 'smører', 'babyolie']\n",
      "['Bizar', 'opladning', 'Derfor', 'smører', 'babyolie']\n",
      "['Når', 'betaler', 'regningen']\n",
      "['Når', 'betaler', 'regningen']\n",
      "['Nye', 'coronatiltag', 'Føles', 'grænseoverskridende']\n",
      "['Nye', 'coronatiltag', 'Føles', 'grænseoverskridende']\n",
      "['Så', 'farlige', 'hestehuller']\n",
      "['Så', 'farlige', 'hestehuller']\n",
      "['Årets', 'bedste', 'Lundgaard', 'Undskyld', 'gutter']\n",
      "['Årets', 'bedste', 'Lundgaard', 'Undskyld', 'gutter']\n"
     ]
    }
   ],
   "source": [
    "x=len(df1)\n",
    "list=[]\n",
    "for i  in range(x):\n",
    "    \n",
    "    df1.dropna(subset = [\"2\"], inplace=True)\n",
    "    df=df1.reset_index(drop=True)\n",
    "    \n",
    "    x=preprocess(df['2'][i])\n",
    "    list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Statsministeren\".isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
