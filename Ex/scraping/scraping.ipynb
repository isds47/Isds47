{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "import requests\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "# similarity/distance measures\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "path = os.path.join(dirname, 'chromedriver')\n",
    "driver = webdriver.Chrome(executable_path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemer med en cookie pop-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 out of 100,000\n",
      "there are now 1530 urls\n",
      "200 out of 100,000\n",
      "there are now 2610 urls\n",
      "300 out of 100,000\n",
      "there are now 3750 urls\n",
      "400 out of 100,000\n",
      "there are now 4860 urls\n",
      "500 out of 100,000\n",
      "there are now 6060 urls\n",
      "600 out of 100,000\n",
      "there are now 7350 urls\n",
      "700 out of 100,000\n",
      "there are now 8700 urls\n",
      "800 out of 100,000\n",
      "there are now 9720 urls\n",
      "900 out of 100,000\n",
      "there are now 10680 urls\n",
      "1000 out of 100,000\n",
      "there are now 11550 urls\n",
      "1100 out of 100,000\n",
      "there are now 12540 urls\n",
      "1200 out of 100,000\n",
      "there are now 13470 urls\n",
      "1300 out of 100,000\n",
      "there are now 14490 urls\n",
      "1400 out of 100,000\n",
      "there are now 15330 urls\n",
      "1500 out of 100,000\n",
      "there are now 16380 urls\n",
      "1600 out of 100,000\n",
      "there are now 17460 urls\n",
      "1700 out of 100,000\n",
      "there are now 18360 urls\n",
      "1800 out of 100,000\n",
      "there are now 19350 urls\n",
      "1900 out of 100,000\n",
      "there are now 20370 urls\n",
      "2000 out of 100,000\n",
      "there are now 21600 urls\n",
      "2100 out of 100,000\n",
      "there are now 22680 urls\n",
      "2200 out of 100,000\n",
      "there are now 23700 urls\n",
      "2300 out of 100,000\n",
      "there are now 24630 urls\n",
      "2400 out of 100,000\n",
      "there are now 25740 urls\n",
      "2500 out of 100,000\n",
      "there are now 26610 urls\n",
      "2600 out of 100,000\n",
      "there are now 28170 urls\n",
      "2700 out of 100,000\n",
      "there are now 29250 urls\n",
      "2800 out of 100,000\n",
      "there are now 30330 urls\n",
      "2900 out of 100,000\n",
      "there are now 32310 urls\n",
      "3000 out of 100,000\n",
      "there are now 33360 urls\n",
      "3100 out of 100,000\n",
      "there are now 34830 urls\n",
      "3200 out of 100,000\n",
      "there are now 36330 urls\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "# Finds and extracts all urls from a search term\n",
    "search_term = 'a'\n",
    "url = f'https://ekstrabladet.dk/find/?g=true&q={search_term}'\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "# more comments button\n",
    "next_page = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/div[4]/div[1]/div/div/div/div[32]/button')\n",
    "# show all comment\n",
    "\n",
    "loops = 0\n",
    "url_pattern = 'https:\\/\\/[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}[-a-zA-Z0-9@:%_\\+.~#?&//=;]*'\n",
    "for i in range(100000): # how many times to press next\n",
    "    loops += 1\n",
    "    try:\n",
    "        next_page.click() # when this shows an error, i.e. no more comments, it breaks.\n",
    "        time.sleep(0.5)\n",
    "        if loops %100 == 0: # save every 100 loops\n",
    "            # get soup\n",
    "            search_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            articles = search_soup.find_all(class_='flex-item mar-l--b width-1of1') # links soup\n",
    "            print(f'{loops} out of 100,000')\n",
    "            urls = []\n",
    "            \n",
    "            for i in range(len(articles)):\n",
    "                urls.append(re.findall(url_pattern, str(articles[i]))) # save url\n",
    "            \n",
    "            print(f'there are now {len(urls)} urls')\n",
    "            \n",
    "            # save to cvs\n",
    "            df_urls = pd.DataFrame(data=urls)\n",
    "            df_urls.to_csv(\"urls.csv\", index=False)\n",
    "    except: # if there are no more articles\n",
    "        print('stop') # stop :I\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3254 out of 100,000\n",
      "there are now 37230 urls\n"
     ]
    }
   ],
   "source": [
    "# get soup\n",
    "search_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "articles = search_soup.find_all(class_='flex-item mar-l--b width-1of1') # links soup\n",
    "print(f'{loops} out of 100,000')\n",
    "urls = []\n",
    "\n",
    "for i in range(len(articles)):\n",
    "    urls.append(re.findall(url_pattern, str(articles[i]))) # save url\n",
    "\n",
    "print(f'there are now {len(urls)} urls')\n",
    "\n",
    "# save to cvs\n",
    "df_urls = pd.DataFrame(data=urls)\n",
    "df_urls.to_csv(\"urls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save soup\n",
    "with open(\"search_html.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(soup.get_text())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load urls\n",
    "urls = pd.read_csv(\"urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scrape headlines, number of comments, and time.\n",
    "'''\n",
    "list1 = []\n",
    "i = 0\n",
    "n = 10 # number of articles to run through\n",
    "for url in urls['0']:\n",
    "    no_comments = False\n",
    "    data_list = []\n",
    "    i +=1\n",
    "    if i == n: # break after n articles\n",
    "        break\n",
    "    \n",
    "    data_list.append(url) # save url to df\n",
    "    driver.get(url) # open the current url\n",
    "    time.sleep(0.5)\n",
    "    url_soup = BeautifulSoup(driver.page_source, 'lxml') # save soup\n",
    "    \n",
    "    \n",
    "    # get ammount of comments\n",
    "    while True:\n",
    "        try:\n",
    "            comments = url_soup.find_all(id='fnTalkCommentText') # div with the comment link\n",
    "            comments = re.findall('\\d+',comments[0].text)\n",
    "            data_list.append(int(comments[0]))\n",
    "            break\n",
    "        except:\n",
    "            data_list.append(np.nan)\n",
    "            no_comments = True\n",
    "            break\n",
    "    \n",
    "    # skips url if no comments\n",
    "    if no_comments: \n",
    "        continue\n",
    "\n",
    "    \n",
    "    # get headline\n",
    "    while True:\n",
    "        try:\n",
    "            headline = url_soup.find_all(class_='art-title') # div with the comment link\n",
    "            headline = re.sub('\\\\n {16}','',headline[0].text)\n",
    "            data_list.append(headline)\n",
    "            break\n",
    "        except:\n",
    "            data_list.append(np.nan)\n",
    "            break\n",
    "            \n",
    "    # get date\n",
    "    while True:\n",
    "        try:\n",
    "            date = url_soup.find(class_='eb-row article-timestamp').get_text()\n",
    "            date=re.findall(\"\\d{2}. \\w+. \\d{4}\",date)\n",
    "            data_list.append(date[0])\n",
    "            break\n",
    "        except:\n",
    "            data_list.append(np.nan)\n",
    "            break\n",
    "    \n",
    "    # save to list of lists\n",
    "    list1.append(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to df and drop nan\n",
    "df = pd.DataFrame(list1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [19. aug. 2020]\n",
       "2    [19. aug. 2020]\n",
       "5    [19. aug. 2020]\n",
       "6    [19. aug. 2020]\n",
       "7    [19. aug. 2020]\n",
       "8    [19. aug. 2020]\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer has a build-in tokenizer and lowercases by default. Also has an option to remove stopwords.\n",
    "vectorizer = CountVectorizer()\n",
    "# However, you can override the default tokenization with your own defined function, like so:\n",
    "#vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# fit and transform train\n",
    "df_bow = vectorizer.fit_transform(df[1].values)\n",
    "# Only tranform test: never fitting your vectorizer on the test set (it is cheating). OOV words are handled automatically be sklearn's vectorizer.\n",
    "#X_test_bow = vectorizer.transform(df_test.review.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 381)\n",
      "381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x381 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_bow.shape)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "df_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "df_tfidf = tfidf.fit_transform(df[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar: [ 3 51 33 74 45 26 21 22 23 24]\n",
      "least similar [46 54 47 48 49 50 52 53  0]\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "cosine_similarities = linear_kernel(df_tfidf[3], df_tfidf).flatten()\n",
    "\n",
    "indices = cosine_similarities.argsort()[::-1] # in descending order \n",
    "print(\"most similar:\",indices[:10])\n",
    "print(\"least similar\", indices[-9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Officielt: Tager over i FC Barcelona\n",
      "\n",
      "most similar:  Barcelonas præsident melder Ronald Koeman tæt på FC Barcelona\n",
      "\n",
      "least similar:  Fængsel skal tage imod coronasmittet\n"
     ]
    }
   ],
   "source": [
    "print(df[1].values[3])\n",
    "print()\n",
    "print(\"most similar: \", df[1].values[51])\n",
    "print()\n",
    "print(\"least similar: \", df[1].values[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Saves the comments in a list.\n",
    "Does not save arcticle headline etc.\n",
    "'''\n",
    "# open comments\n",
    "comments= []\n",
    "i = 0 # to only run loop x times\n",
    "for i, url in enumerate(urls):\n",
    "    # initiate check variables\n",
    "    i2 = 0\n",
    "    i3 = 0\n",
    "    no_comments = False\n",
    "    \n",
    "    i +=1\n",
    "    print(f\"i = {i}\")\n",
    "    if i == 50: # break after 10 articles\n",
    "        break\n",
    "    \n",
    "    print(url[0])\n",
    "    driver.get(url[0]) # open the current url\n",
    "    time.sleep(5)\n",
    "    '''\n",
    "    Tries to find the show more comments button.\n",
    "    break after 10 seconds of trying\n",
    "    '''\n",
    "    while True: # infinite loop\n",
    "        try:\n",
    "            # show comments button\n",
    "            show_comments = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/div[4]/div[3]/div[2]/div')\n",
    "            show_comments.click() # click it\n",
    "            time.sleep(5)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(1) # waits a total of 10 seconds i2*10 for the page to load\n",
    "            i2 +=1\n",
    "            print(f\"i2 = {i2}\")\n",
    "            if i2 == 5:\n",
    "                no_comments = True\n",
    "                break\n",
    "   \n",
    "    if no_comments: # next article\n",
    "        continue\n",
    "    '''\n",
    "    Tries to find the comment url.\n",
    "    if the url is blank wait and try again.\n",
    "    break after 10 seconds\n",
    "    '''\n",
    "    while True: # infinite loop\n",
    "        try:\n",
    "            show_comments_soup = BeautifulSoup(driver.page_source, 'lxml') # save soup\n",
    "            comment_link = show_comments_soup.find_all(id='talkStream_iframe') # div with the comment link\n",
    "            comment_url = re.findall(url_pattern, str(comment_link)) # save the comment url\n",
    "            print(comment_url)\n",
    "            if comment_url == []:\n",
    "                i3 += 1\n",
    "                print(f\"i3 = {i3}\")\n",
    "                time.sleep(1)\n",
    "                if i3 == 5:\n",
    "                    break\n",
    "                continue\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    # open the comment_url to save comments, break if not possible\n",
    "    while True: # infinite loop\n",
    "        try:\n",
    "            driver.get(comment_url[0])\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    '''\n",
    "    tries to show all comments.\n",
    "    when there are no more next_comments it breaks, also breaks if the button does not exist\n",
    "    '''\n",
    "    while True: # infinite loop\n",
    "        try:\n",
    "            # more comments button\n",
    "            next_comment = driver.find_element_by_xpath('/html/body/div[1]/div/div[2]/div/div/div/div[3]/div[2]/div/div/div/div/div[3]/button')\n",
    "            next_comment.click()\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            break\n",
    "    \n",
    "    '''\n",
    "    save the extracted comments to list.\n",
    "    breaks if no comments were extracted\n",
    "    '''\n",
    "    while True:\n",
    "        try:\n",
    "            comment_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            comments_ = comment_soup.find_all(class_=\"talk-plugin-rich-text-text CommentContent__content___ZGv1q\")\n",
    "            if comments_ == []:\n",
    "                break\n",
    "            else:\n",
    "                comments.append(comments_)\n",
    "                break\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-91a946dcd53a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'comments' is not defined"
     ]
    }
   ],
   "source": [
    "comments[3][4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the comments:\n",
    "df_comments = pd.DataFrame(data=comments)\n",
    "df_comments.to_csv(\"comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
